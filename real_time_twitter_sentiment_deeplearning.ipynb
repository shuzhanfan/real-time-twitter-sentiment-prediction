{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuzhan Fan, Oct 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "from time import time\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from scipy.stats import hmean, norm\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, concatenate, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_col_names = ['sentiment','id','date','query_string','user','text']\n",
    "df1 = pd.read_csv(\"data/sentiment140_training.csv\", header=None, names=df1_col_names)\n",
    "df2 = pd.read_csv(\"data/nieksanders_full.csv\", header=\"infer\")\n",
    "df3 = pd.read_csv(\"data/umich_training.txt\", header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['id','date','query_string','user'], axis=1, inplace=True)\n",
    "df2.drop(['Topic','TweetId','TweetDate'], axis=1, inplace=True)\n",
    "df2.columns = ['sentiment', 'text']\n",
    "df3.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map sentiment value to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sentiment'] = df1['sentiment'].apply(lambda x: 1 if x==4 else x)\n",
    "df2 = df2.loc[(df2['sentiment'] == 'positive') | (df2['sentiment'] == 'negative')]\n",
    "df2['sentiment'] = df2['sentiment'].apply(lambda x: 1 if x==\"positive\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat three data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3])\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_pattern = r\"https?://[^ ]+\"\n",
    "www_pattern = r\"www.[^ ]+\"\n",
    "user_pattern = r\"@[A-Za-z0-9_]+\"\n",
    "url_user_pattern = r\"|\".join((html_pattern, www_pattern, user_pattern))\n",
    "negations = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "             \"don't\":\"do not\", \"doesn't\":\"does not\", \"didn't\":\"did not\",\n",
    "             \"haven't\":\"have not\", \"hasn't\":\"has not\", \"hadn't\":\"had not\",\n",
    "             \"won't\":\"will not\", \"wouldn't\":\"would not\",\n",
    "             \"can't\":\"can not\", \"couldn't\":\"could not\",\n",
    "             \"shouldn't\":\"should not\",\n",
    "             \"mightn't\":\"might not\",\n",
    "             \"mustn't\":\"must not\"}\n",
    "negations_pattern = re.compile(r'\\b(' + '|'.join(negations.keys()) + r')\\b')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    soup = BeautifulSoup(tweet, \"lxml\")\n",
    "    souped = soup.get_text()\n",
    "    url_clean = re.sub(url_user_pattern, \"\", souped)\n",
    "    try:\n",
    "        bom_clean = url_clean.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_clean = url_clean\n",
    "    negation_clean = negations_pattern.sub(lambda x: negations[x.group()], bom_clean)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", negation_clean)\n",
    "    lower = letters_only.lower()\n",
    "    words = [x for x in tokenizer.tokenize(lower) if len(x) > 1]\n",
    "    tweet_clean = (\" \".join(words)).strip()\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tweets_cleaned = []\n",
    "for i in xrange(0, len(df)):\n",
    "    if ((i+1) % 100000) == 0:\n",
    "        print \"Tweets %d of %d has been processed\" % ( i+1, len(df) )\n",
    "    tweets_cleaned.append(clean_tweet(df.text[i]))\n",
    "\n",
    "df_cleaned = pd.DataFrame(tweets_cleaned, columns=[\"text\"])\n",
    "df_cleaned[\"target\"] = df.sentiment\n",
    "df_cleaned.to_csv(\"tweets_cleaned.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read cleaned tweets and drop NaN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"tweets_cleaned.csv\", index_col=0)\n",
    "df_clean.dropna(inplace=True)\n",
    "df_clean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot WordCloud for positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df_clean[df_clean[\"target\"]==1]\n",
    "df_negative = df_clean[df_clean[\"target\"]==0]\n",
    "np_positive = df_positive.text.values\n",
    "np_negative = df_negative.text.values\n",
    "text_positive = pd.Series(np_positive).str.cat(sep=\" \")\n",
    "text_negative = pd.Series(np_negative).str.cat(sep=\" \")\n",
    "wc_positive = WordCloud(width=1800, height=1000, max_font_size=250).generate(text_positive)\n",
    "wc_negative = WordCloud(width=1800, height=1000, max_font_size=250).generate(text_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc_positive, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wc_negative, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(df_clean.text)\n",
    "mat_positive = count_vectorizer.transform(df_clean[df_clean.target==1].text)\n",
    "mat_negative = count_vectorizer.transform(df_clean[df_clean.target==0].text)\n",
    "tf_positive  = np.sum(mat_positive, axis=0)\n",
    "tf_negative  = np.sum(mat_negative, axis=0)\n",
    "np_positive  = np.squeeze(np.asarray(tf_positive))\n",
    "np_negative  = np.squeeze(np.asarray(tf_negative))\n",
    "df_tf = pd.DataFrame([np_negative,np_positive], columns=count_vectorizer.get_feature_names()).transpose()\n",
    "df_tf.columns = [\"negative\", \"positive\"]\n",
    "df_tf[\"total\"] = df_tf.negative + df_tf.positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_harmonic_mean(x):\n",
    "    pr  = x[\"positive_rate\"]\n",
    "    pfp = x[\"positive_relative_frequency\"]\n",
    "    if pr > 0 and pfp > 0:\n",
    "        return hmean([pr, pfp])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def negative_harmonic_mean(x):\n",
    "    nr  = x[\"negative_rate\"]\n",
    "    nfp = x[\"negative_relative_frequency\"]\n",
    "    if nr > 0 and nfp > 0:\n",
    "        return hmean([nr, nfp])\n",
    "    else:\n",
    "        return 0\n",
    "      \n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "df_tf[\"positive_rate\"] = df_tf[\"positive\"]*1. / df_tf[\"total\"]\n",
    "df_tf['positive_relative_frequency'] = df_tf['positive'] * 1./df_tf['positive'].sum()\n",
    "df_tf[\"positive_rate_normcdf\"]     = normcdf(df_tf[\"positive_rate\"])\n",
    "df_tf[\"positive_relative_frequency_normcdf\"] = normcdf(df_tf[\"positive_relative_frequency\"])\n",
    "df_tf[\"positive_normcdf_harmonic_mean\"]    = hmean([df_tf[\"positive_rate_normcdf\"], df_tf[\"positive_relative_frequency_normcdf\"]])\n",
    "df_tf[\"negative_rate\"]             = df_tf[\"negative\"] * 1. / df_tf[\"total\"]\n",
    "df_tf[\"negative_relative_frequency\"]         = df_tf[\"negative\"] * 1. / df_tf[\"negative\"].sum()\n",
    "df_tf[\"negative_rate_normcdf\"]     = normcdf(df_tf[\"negative_rate\"])\n",
    "df_tf[\"negative_relative_frequency_normcdf\"] = normcdf(df_tf[\"negative_relative_frequency\"])\n",
    "df_tf[\"negative_normcdf_harmonic_mean\"]    = hmean([df_tf[\"negative_rate_normcdf\"], df_tf[\"negative_relative_frequency_normcdf\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf.drop(['positive_rate', 'negative_rate', 'positive_relative_frequency', 'negative_relative_frequency', 'positive_rate_normcdf', 'negative_rate_normcdf', 'positive_relative_frequency_normcdf', 'negative_relative_frequency_normcdf'], axis=1, inplace=True)\n",
    "df_tf.to_csv(\"term_frequencies.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot top 30 positive and negative tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.read_csv(\"term_frequencies.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coords = np.arange(20)\n",
    "df_tf_positive_sorted = df_tf.sort_values(by=\"positive_normcdf_harmonic_mean\", ascending=False)[\"positive_normcdf_harmonic_mean\"][:20]\n",
    "df_tf_negative_sorted = df_tf.sort_values(by=\"negative_normcdf_harmonic_mean\", ascending=False)[\"negative_normcdf_harmonic_mean\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.barh(-y_coords, df_tf_positive_sorted, align=\"center\", alpha=0.8)\n",
    "plt.yticks(-y_coords, df_tf_positive_sorted.index, rotation=\"horizontal\", fontsize=14)\n",
    "plt.xlabel(\"Positive_normcdf_hmean value\", fontsize=14)\n",
    "plt.xlim((0.60, 0.91))\n",
    "plt.ylabel(\"Words\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.barh(-y_coords, df_tf_negative_sorted, align=\"center\", alpha=0.8)\n",
    "plt.yticks(-y_coords, df_tf_negative_sorted.index, rotation=\"horizontal\", fontsize=14)\n",
    "plt.xlabel(\"Negative_normcdf_hmean value\", fontsize=14)\n",
    "plt.xlim((0.70, 0.94))\n",
    "plt.ylabel(\"Words\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "cpus = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"tweets_cleaned.csv\", index_col=0)\n",
    "df_clean.dropna(inplace=True)\n",
    "df_clean.reset_index(drop=True, inplace=True)\n",
    "X = df_clean.text\n",
    "y = df_clean.target\n",
    "X_train, X_vali_test, y_train, y_vali_test = train_test_split(X, y, test_size=0.02, random_state=8081)\n",
    "X_vali, X_test, y_vali, y_test = train_test_split(X_vali_test, y_vali_test, test_size=0.5, random_state=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_documents(tweets, prefix):\n",
    "    tagged_tweets = []\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        tagged_tweets.append(TaggedDocument(t.split(), [prefix + str(i)]))\n",
    "    return tagged_tweets\n",
    "X_tagged = tag_documents(X, 'tweet_')\n",
    "\n",
    "def tweet_word2vec(tweet, size, model, aggr):\n",
    "    tweet_vecs = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            tweet_vecs += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggr == 'mean':\n",
    "        if count != 0:\n",
    "            tweet_vecs /= count\n",
    "        return tweet_vecs\n",
    "    elif aggr == 'sum':\n",
    "        return tweet_vecs\n",
    "    \n",
    "def tweet_word2vec_concat(tweet, size, model1, model2, aggr):\n",
    "    tweet_vecs = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            tweet_vecs += np.append(model1[word], model2[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggr == 'mean':\n",
    "        if count != 0:\n",
    "            tweet_vecs /= count\n",
    "        return tweet_vecs\n",
    "    elif aggr == 'sum':\n",
    "        return tweet_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cpus, alpha=0.05, min_alpha=0.05)\n",
    "word2vec_cbow.build_vocab([x.words for x in tqdm(X_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(30):\n",
    "    word2vec_cbow.train(utils.shuffle([x.words for x in tqdm(X_tagged)]), total_examples=len(X_tagged), epochs=1)\n",
    "    word2vec_cbow.alpha -= 0.002\n",
    "    word2vec_cbow.min_alpha = word2vec_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2vs_cbow_mean = scale(np.concatenate([tweet_word2vec(tweet, 100, word2vec_cbow,'mean') for tweet in X_train]))\n",
    "X_vali_w2vs_cbow_mean  = scale(np.concatenate([tweet_word2vec(tweet, 100, word2vec_cbow,'mean') for tweet in X_vali]))\n",
    "X_train_vali_w2vs_cbow_mean = (X_train_w2vs_cbow_mean, X_vali_w2vs_cbow_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Skip-Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_skgr = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cpus, alpha=0.05, min_alpha=0.05)\n",
    "word2vec_skgr.build_vocab([x.words for x in tqdm(X_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for _ in range(30):\n",
    "    word2vec_skgr.train(utils.shuffle([x.words for x in tqdm(X_tagged)]), total_examples=len(X_tagged), epochs=1)\n",
    "    word2vec_skgr.alpha -= 0.002\n",
    "    word2vec_skgr.min_alpha = word2vec_skgr.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2vs_skgr_mean = scale(np.concatenate([tweet_word2vec(tweet, 100, word2vec_skgr,'mean') for tweet in X_train]))\n",
    "X_vali_w2vs_skgr_mean  = scale(np.concatenate([tweet_word2vec(tweet, 100, word2vec_skgr,'mean') for tweet in X_vali]))\n",
    "X_train_vali_w2vs_skgr_mean = (X_train_w2vs_skgr_mean, X_vali_w2vs_skgr_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Word2Vec vectors from CBOW and Skim-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2vs_concat_mean = scale(np.concatenate([tweet_word2vec_concat(tweet, 200, word2vec_cbow, word2vec_skgr, 'mean') for tweet in X_train]))\n",
    "X_vali_w2vs_concat_mean  = scale(np.concatenate([tweet_word2vec_concat(tweet, 200, word2vec_cbow, word2vec_skgr, 'mean') for tweet in X_vali]))\n",
    "X_train_vali_w2vs_concat_mean = (X_train_w2vs_concat_mean, X_vali_w2vs_concat_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save two Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cbow.save(\"word2vec_cbow.w2v\")\n",
    "word2vec_skgr.save(\"word2vec_skgr.w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tweet vectors with three Word2Vec methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_vali_w2vs_cbow_mean, 'X_train_vali_w2vs_cbow_mean.pkl') \n",
    "joblib.dump(X_train_vali_w2vs_skgr_mean, 'X_train_vali_w2vs_skgr_mean.pkl') \n",
    "joblib.dump(X_train_vali_w2vs_concat_mean, 'X_train_vali_w2vs_concat_mean.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweet vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vali_w2vs_cbow_mean   = joblib.load(\"X_train_vali_w2vs_cbow_mean.pkl\")\n",
    "X_train_vali_w2vs_skgr_mean   = joblib.load(\"X_train_vali_w2vs_skgr_mean.pkl\")\n",
    "X_train_vali_w2vs_concat_mean = joblib.load(\"X_train_vali_w2vs_concat_mean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load two Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cbow = KeyedVectors.load(\"word2vec_cbow.w2v\")\n",
    "word2vec_skgr = KeyedVectors.load(\"word2vec_skgr.w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_w2vs_dict = {}\n",
    "for word in word2vec_cbow.wv.vocab.keys():\n",
    "    vocab_w2vs_dict[word] = np.append(word2vec_cbow.wv[word], word2vec_skgr.wv[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_vali  = tokenizer.texts_to_sequences(X_vali)\n",
    "X_train_sequences = pad_sequences(sequences_train, maxlen=50)\n",
    "X_vali_sequences  = pad_sequences(sequences_vali, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_w2v_matrix = np.zeros((100000, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= 100000:\n",
    "        continue\n",
    "    vocab_w2v_vector = vocab_w2vs_dict.get(word)\n",
    "    if vocab_w2v_vector is not None:\n",
    "        vocab_w2v_matrix[i] = vocab_w2v_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vocab_w2v_matrix, 'vocab_w2v_matrix.pkl') \n",
    "joblib.dump(X_train_sequences, 'X_train_sequences.pkl') \n",
    "joblib.dump(X_vali_sequences, 'X_vali_sequences.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabulary Word2Vec matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_w2v_matrix = joblib.load(\"vocab_w2v_matrix.pkl\")\n",
    "X_train_sequences = joblib.load(\"X_train_sequences.pkl\")\n",
    "X_vali_sequences = joblib.load(\"X_vali_sequences.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"tweets_cleaned.csv\", index_col=0)\n",
    "df_clean.dropna(inplace=True)\n",
    "df_clean.reset_index(drop=True, inplace=True)\n",
    "X = df_clean.text\n",
    "y = df_clean.target\n",
    "X_train, X_vali_test, y_train, y_vali_test = train_test_split(X, y, test_size=0.02, random_state=8081)\n",
    "X_vali, X_test, y_vali, y_test = train_test_split(X_vali_test, y_vali_test, test_size=0.5, random_state=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(100000, 200, weights=[vocab_w2v_matrix], input_length=50, trainable=True))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(64))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Activation(\"relu\"))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.add(Activation(\"sigmoid\"))\n",
    "adam       = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model_file = \"best_LSTM_model.{epoch:02d}-{val_acc:.5f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(best_lstm_model_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\")\n",
    "reduce_lr  = ReduceLROnPlateau(monitor=\"val_acc\", factor=0.5, patience=2, min_lr=0.000001)\n",
    "lstm_model.fit(X_train_sequences, y_train, batch_size=128, epochs=5, shuffle=True, callbacks=[checkpoint, early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input      = Input(shape=(50,), dtype='int32')\n",
    "embedding  = Embedding(100000, 200, weights=[vocab_w2v_matrix], input_length=50, trainable=True)(input)\n",
    "conv1      = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(embedding)\n",
    "conv1_pool = GlobalMaxPooling1D()(conv1)\n",
    "conv2      = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(embedding)\n",
    "conv2_pool = GlobalMaxPooling1D()(conv2)\n",
    "conv3      = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(embedding)\n",
    "conv3_pool = GlobalMaxPooling1D()(conv3)\n",
    "conv123_concat = concatenate([conv1_pool, conv2_pool, conv3_pool], axis=1)\n",
    "full       = Dense(256, activation='relu')(conv123_concat)\n",
    "dropout    = Dropout(0.2)(full)\n",
    "output     = Dense(1)(dropout)\n",
    "output     = Activation('sigmoid')(output)\n",
    "cnn_model      = Model(inputs=[input], outputs=[output])\n",
    "adam       = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model_file = \"best_CNN_model.{epoch:02d}-{val_acc:.5f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(best_cnn_model_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\")\n",
    "reduce_lr  = ReduceLROnPlateau(monitor=\"val_acc\", factor=0.5, patience=2, min_lr=0.000001)\n",
    "cnn_model.fit(X_train_sequences, y_train, batch_size=128, epochs=5, validation_data=(X_vali_sequences, y_vali), callbacks = [checkpoint, early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other machine learning baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vali_w2vs_concat_mean = joblib.load(\"X_train_vali_w2vs_concat_mean.pkl\")\n",
    "X_train_w2vs_concat_mean, X_vali_w2vs_concat_mean = X_train_vali_w2vs_concat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic     = LogisticRegression()\n",
    "decision_tree  = DecisionTreeClassifier()\n",
    "svm_linear     = LinearSVC()\n",
    "random_forest  = RandomForestClassifier()\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "ml_models = [logistic, decision_tree, svm_linear, random_forest, gradient_boost]\n",
    "ml_names  = [\"Logistic regression\", \"Decision tree\", \"Linear SVM\", \"Random forest\", \"Gradient Boosting\"]\n",
    "ml_models_names = zip(ml_models, ml_names)\n",
    "\n",
    "def compare_models(model_list, xtrain, ytrain, xtest, ytest):\n",
    "    for model, name in model_list:\n",
    "        print \"Validation results for {}\".format(name)\n",
    "        print model\n",
    "        time_start = time()\n",
    "        model_fit = model.fit(xtrain, ytrain)\n",
    "        y_pred    = model_fit.predict(xtest)\n",
    "        time_end = time()\n",
    "        accuracy = accuracy_score(ytest, y_pred) \n",
    "        print \"accuracy: {0:.2f}%\".format(accuracy*100) \n",
    "        print \"model running time: {0:.2f}s\".format(time_end-time_start) \n",
    "        print \" \"\n",
    "compare_models(ml_models_names, X_train_w2vs_concat_mean, y_train, X_vali_w2vs_concat_mean, y_vali)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
